<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S79B41WR96"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S79B41WR96');
</script>

<meta  http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>
Aukosh Jagannath
</title>
<link rel="stylesheet" type="text/css" href="jag.css" /> 
<!--
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
<?php
if( isset( $_COOKIE['user_is_admin12312341212'] ) ) {
    echo 'ga(\'set\', \'dimension1\', \'true\');';
}
?>
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
-->
<!--<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytic\
s.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

  </script>-->

</head>
<body>
<div class="container">

<div class="header">
<h1>
Aukosh Jagannath

</h1>
</div>
<div class = "menu">
<a href="./">home</a> &nbsp;
<a href="./research.html">research</a> &nbsp;
<a href="./teaching.html"><b>teaching</b></a>&nbsp;
<a href="./aukoshJagannathcv.pdf"> CV </a>

</div>
<br>
<b> Course:</b> STAT 946 &mdash; Adv Topics: Mathematical Foundations of Deep Learning<br><br><br>

<b> Blurb:</b> 
  The goal of this course is to introduce and explore some of the recent theoretical advances that 
  aim to understand modern deep learning methods and training regimes. <br><br>

  Topics may include: Universal approximations, Uniform convergence, Benign overparamterization, functional limit theory/scaling limits,
  NTK, comparison to kernel methods, Training dynamics and related phenomenology (Implicit regularization, Training regimes, sample complexity, 
  Mean-Field/hydrodynamic limits, DMFT/Effective Dynamics), Loss landscapes and geometry, transformers, diffusion models.<br><br>

  There will be a heavy focus on the analysis of training dynamics (sample complexity and scaling limits).  
  The course will be interspersed with primers on important tools and techniques from probability theory such as 
  concentration of measure, random matrix theory, stochastic analysis, and related ideas inspired by statistical physics.<br><br>

  This will be a fast-paced, research-level, seminar-style course. We will be learning as a group. 
  Students will play an active role in the course: as the course progresses students will pick up and explore these 
  or other topics to catch us all up!
  <br><br>

The following is tentative and comments/suggested reading are welcome!<br><br>

<b> Lectures</b><br> <br>
<b> Week 1:</b> Approximation theory<br>
  <ul style="list-style-type:none;">
    <li> <b> Lec 1:</b> Universal Approximation and Depth Separation </li>
    <li><b>Reading:</b> <a href=https://mjt.cs.illinois.edu/dlt/two.pdf>Telgarsky's Deep Learning Theory notes, Chap 1,4</a></li>
  </ul>
<b> Week 2:</b> Uniform convergence <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 2: </b> Generalization and Rademacher Complexity </li>
  <li> <b> Lec 3:</b> Vacuous bounds and the need for a new approach </li>
  <li><b>Reading:</b></li> 
  <ul>
    <li>(BMR) P. Bartlett, A Montanari, and A Rakhlin, <a href=https://www.cambridge.org/core/journals/acta-numerica/article/deep-learning-a-statistical-viewpoint/7BCB89D860CEDDD5726088FAD64F2A5A>
  Deep Learning: A statistical view point</a> Sec 2 </li>
   <!-- <li> M. Mohri, <a href=https://cs.nyu.edu/~mohri/mlbook/> Foundations of Machine Learning</a> Chap 3</li>-->
    <li> M. Wainwright, <a href="https://doi.org/10.1017/9781108627771">High-dimensional Statistics</a> Chap 4</li>
    <li> V. Nagarajan and J. Zico Kolter, <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf">Uniform convergence may be unable to explain generalization in deep learning</a>,
      NeurIPS 2019</li>
    <li> J. Negrea, G. K. Dziugaite, and D. Roy, <a href="http://proceedings.mlr.press/v119/negrea20a/negrea20a.pdf"> In Defense of Uniform Convergence</a> ICML 2020</li>
  <!--<li> S. Ben-David, S. Shalev-Schwarz "Understanding Machine Learning: From Theory to Algorithms" </li>-->
  </ul>
</ul>
<b> Week 3:</b> Implicit regularization and benign overparamterization<br>
<ul style="list-style-type:none;">
  <li> <b> Lec 4:</b> Implicit Regularization </li>
  <li> <b> Lec 5:</b> Interpolation does not imply poor generalzation </li>
  <li><b>Reading:</b></li>
  <ul>
    <li> Soudry-Hoffer-Nacson-Gunasekar-Srebro <a href="https://www.jmlr.org/papers/volume19/18-188/18-188.pdf">The Implicit Bias of Gradient Descent on Separable Data</a> JMLR 2018</li>
    <li> <a href=https://mjt.cs.illinois.edu/dlt/index.pdf>Telgarsky's Deep Learning Theory notes, Chap 10</a></li>
    <li> M. Belkin, A. Rakhlin, A.B. Tsybakov <a href="https://proceedings.mlr.press/v89/belkin19a.html">Does data interpolation contradict statistical optimality</a> AISTATS 2019</li>
  </ul>
</ul>
<b> Week 4:</b> RMT and double descent <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 6:</b> Random Matrix Theory: a primer</li>
  <li> <b> Lec 7:</b> High-dimensional ridgeless least squares</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>G. W. Anderson, A. Guionnet, and O. Zeitouni, <a href="http://www.wisdom.weizmann.ac.il/~zeitouni/cupbook.pdf"> An Introduction to Random Matrices</a></li>
    <li>T. Hastie, A. Montanari, S. Rosset, and RJ Tibshirani, <a href="https://arxiv.org/pdf/1903.08560">Surprises in High-dimensional Ridgeless least squares interpolation </a> Ann. Stats 2022</li>
  </ul>
</ul>
<b> Week 5:</b> NTK and Lazy training <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 8:</b> The Neural Tangent Kernel</li>
  <li> <b> Lec 9:</b> Lazy training</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>A. Jacot, F. Gabriel, and C. Hongler, <a href="https://papers.nips.cc/paper_files/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html"> Neural Tangent Kernel: Convergence and Generalization in Neural Networks</a> NeurIPS 2018</li>
    <li> BMR Sec 5 </li>
  </ul>
</ul>
<b> Week 6:</b> Infinite Width limits <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 10:</b> Guest Lecture</li>
  <li> <b> Lec 11:</b> Neural networks as interacting particle systems</li>

  <li><b>Reading:</b></li>
  <ul>
    <li>S. Mei, A. Montanari, P.-M. Nguyen,  <a href="https://web.stanford.edu/~montanar/RESEARCH/FILEPAP/mean_field.pdf"> A Mean Field View of the Landscape of Two-Layer Neural Networks </a> PNAS 2018</li>
  </ul>
</ul>
<b> Week 7: Reading Week</b> <br>
<b> Week 8:</b> Sample complexity and scaling limits <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 12:</b> Sample complexity and the information exponent</li>
  <li> <b> Lec 13:</b> Effective dynamics</li>
  <li><b>Reading:</b></li>
  <ul>
    <li> G. Ben Arous, R. Gheissari, A. Jagannath <a href="https://jmlr.org/papers/volume22/20-1288/20-1288.pdf">  Online stochastic gradient descent on non-convex losses from high-dimensional inference</a> JMLR 2021</li>
    <li> G. Ben Arous, R. Gheissari, A. Jagannath <a href = "https://onlinelibrary.wiley.com/doi/10.1002/cpa.22169">High-dimensional limit theorems for SGD: Effective dynamics and critical scaling</a> Comm Pure Appl Math 2024, NeurIPS 2022</li>
  </ul>
</ul>
<b> Week 9:</b> Spectral alignment and Transformers <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 14:</b> Spectral alignment</li>
  <li> <b> Lec 15:</b> Transformers </li>
  <li><b>Reading:</b></li>
  <ul>
    <li> G. Ben Arous, R. Gheissari, J. Huang, A. Jagannath <a href="https://openreview.net/pdf?id=MHjigVnI04">High-dimensional SGD aligns with emerging outlier eigenspaces </a>  ICLR 2024 </li>
    <li> B. Geshkovski, C. Letrouit, Y. Polyanskiy, and P. Rigollet, <a href="https://arxiv.org/pdf/2312.10794"> A mathematical perspective on transformers</a></li>
  </ul>
</ul>
<b> Week 10:</b> CANCELLED <br>
<b> Week 11:</b> Diffusion Models + Student presentations<br>
<ul style="list-style-type:none;">
  <li> <b> Lec 16:</b> Diffusion models</li>
  <li> <b> Presentation 1:</b> Valentio, Learning GMMs using DDPM </li>
  <li><b>Reading:</b></li>
  <ul>
    <li> A. Montanari, <a href="https://arxiv.org/pdf/2305.10690"> Sampling, Diffusions, and Stochastic Localization</a></li>
    <li> K. Shah, S. Chen, A. Klivans, <a href="https://arxiv.org/pdf/2307.01178">Learning Mixtures of Gaussians Using the DDPM Objective </a> </li>
  </ul>
</ul>
<b> Week 12:</b> Student Presentations <br>
<ul style="list-style-type:none;">
  <li> <b> Presentation 2</b> Theo, Leap Complexity </li>
  <li> <b> Presentation 3</b> Varnan, Benefits of Reuse</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>TBA </li>
  </ul>
</ul>
<b> Week 13:</b> Student Presentations <br>
<ul style="list-style-type:none;">
  <li> <b> Presentation 4</b> Parsa, Scaling limits of GLMs</li>
  <li> <b> Presentation 5</b> Juju, Neural Covariance</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>TBA </li>
  </ul>
</ul>
<b> Week 14:</b> Student Presentations <br>
<ul style="list-style-type:none;">
  <li> <b> Presentation 6:</b> Sammy, Edge of Stability</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>TBA</li>
  </ul>
</ul>



<!--
<br>
<b> Course</b>: STAT 240 - Probability theory (Advanced level) <br>
<b> Section</b>: TBA <br>
<b> Room: </b> TBA <br>
<b> Office Hours: </b> TBA <br><br>
<b> Blurb: </b><br> 
Probability theory nominally concerns phenomena with uncertain outcomes. 
While its origins can be traced back to gambling rooms and insurance programs,
it now plays a central role in many branches of the sciences and engineering, from biology and physics, to signal processing and data science. During this course we will learn how to construct and analyze probabilistic models as well as understand and prove universal properties of such models. 
<br><br>

This course will be a rigorous mathematical introduction to  probability theory. It will be proof-based, will be at a substantially higher level than Stat 230, and much faster paced. We should be covering (not exhaustive): random variables (discrete, continuous, multi-dimensional distributions, etc.), conditional probability, characteristic and moment generating functions, the law of large numbers and central limit theorem, random walks, Poisson Processes, Markov chains, function spaces, and random graphs.
<br><br>


<b>Official Pre-requisites:</b>
Prereq: MATH 137 with a grade of at least 60% or MATH 147; Level at least 1B Honours Mathematics students only. Some mathematical maturity required.<br>
Coreq: MATH 138 or 148.<br>
Antireq: STAT 220, 230

<br><br>
<b> Recommended Textbooks: </b> 
<br>Grimmett and Stirzaker, "Probability and Random Processes" (3rd Ed)
<br>Bertsekas and Tsitsiklis, "Introduction to Probability" (2nd Ed)
<br>Feller, Intro. to Probability Vol. 1
<br>Williams, Probability with Martingales 
<br> <br>

-->

<!--

<br/>
<b> Course</b>: STAT 333 &mdash;  Stochastic Processes I<br/>
<b> Section</b>: Lec 001&mdash; Winter 2022<br/>
<b> Location: </b> DWE 3522<br/>
<b> Time: </b> MW 14.30-15.50
<!--<b> Live-stream Times: </b> 10-11.20 M and 14.30 - 15.50 W <br/>
<b> Office Hours: </b> 4 -5pm W (via Teams) <br/>
<b> Course website: </b> See Learn
<br/><br/>

-->
<!--


<br/>
<b> Course</b>: STAT 946 &mdash;  Mathematics of Data Science (Adv. Topics) <br/>
<b> Section</b>: Lec 001&mdash; Winter 2021<br/>
<b> Location/Medium: </b> Live-stream <br/>
<b> Live-stream Times: </b> TTh 1-2.20pm EST (GMT-5)  <br/>
<b> Office Hours: </b> By appointment<br/>
<b> Course website: </b> See Learn.
<br/><br/>
 
    <b> Blurb: </b>
Data science is an exciting and multidisciplinary field which connects problems in a broad range of fields from statistics and probability, to computation and optimization, to harmonic analysis and dynamical systems. Given inference task, is it possible to compute your favorite estimator on a feasible timescale? Conversely, given a problem from machine learning, what are the fundamental statistical limits of your method? How do you begin to answer these questions when the dimension of the parameter space or the data set is extremely large and has complex (e.g., manifold, graph, group) structure? In these settings many of the classical tools from these fields do not apply and we have to revisit the mathematical foundations. <br/><br/>
  

<b>Syllabus:</b> 

The goal of this course is to quickly orient you toward mathematical problems at the heart of data science. This course will focus on the statistical and computational limits of high-dimensional problems. The first third of the course will serve as an introduction to the basic tools of the course, namely high-dimensional probability and random matrix theory. We will then cover a broad range of applications. Depending on interest, applications may include:<br/>

<ul>
  <li>Dimension reduction</li>
  <li>Sparsity</li>
  <li>Low-rank models: the spiked matrix model for PCA and the BBP transition</li>
  <li> Community detection</li>
  <li> Mean-field methods: approximate message passing, belief propagation, et al </li>
  <li> Neural networks: mean-field limits and NTK for shallow networks, related kernel methods, approximation theory</li>
  <li>Stochastic approximation algorithms, Markov processes, and their mixing properties</li>
  <li>Random optimization problems</li>
  <li>Statistical phase transitions: Statistical—to—computational gaps</li>
</ul><br/>

  <b>Suggested Background:</b> This will be an advanced topics course and will be mathematically rigorous. We will focus on theoretical results regarding both statistical and computational limits. It is strongly recommended that the student have successfully completed at least one advanced course in probability, stochastic processes, or mathematical statistics, as well as courses in analysis and linear algebra. Please contact the instructor to confirm if your background is sufficient. <br/><br/>

<b> Lectures </b><br/><br/>
<b><u>Part I:</u> Fundamentals </b><br/><br/>
<b> Week 1: High-dimensional probability</b>
<ul style="list-style-type:none;">
  <li><b> Lec 1:</b> Surprises in high-dimensions,</li>
  <li><b> Lec 2:</b> Concentration of Gaussians</li>
</ul>
<b>Week 2: High-dimensional probability</b>
<ul style="list-style-type:none;">
  <li><b> Lec 3: </b> Concentration for (Gaussian) random matrices</li>
  <li><b> Lec 4: </b> Isoperimetric, Log-Sobolev, and Poincare inequalities </li>
</ul>
<b>Week 3: PCA and spiked matrix models </b>
<ul style="list-style-type:none;">
  <li><b> Lec 5: </b> Covariance estimation and spiked matrix models</li>
  <li><b> Lec 6: </b> Stieljes transforms and Wigner's theorem</li>
</ul>
<b> Week 4: Spiked matrix models </b>
<ul style="list-style-type:none;">
  <li><b> Lec 7: </b> The Marchenko&mdash;Pastur Law </li>
  <li><b> Lec 8: </b> The Baik-Ben Arous-Peche transition: a short proof 
</ul>

<b> Week 5: Gaussian processes and the M* bound</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 9: </b> Comparison Inequalities </li>
  <li><b> Lec 10: </b> Recovering a vector from a few random measurements </li>
</ul>
<b>Week 6: The escape theorem and exact recovery</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 10: </b> Recovering a vector from a few random measurements</li>
</ul><br/>

<b><u>Part II:</u> Selected recent results</b><br/><br/>
<b> Week 6: Mean-field methods I</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 11: </b>Approximate Message Passing in high-dimensional statistics</li>
</ul>
<b> Week 7: Mean-field methods II</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 12: </b> Approximate Message passing (ctd) </li>
  <li> <b> Lec 13:</b> Mean-field theories, MCMC, free energies </li>
</ul>
<b> Week 8: Neural Networks </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 14: </b> Neural Nets and Approximation theory </li>
  <li> <b> Lec 15:</b> Double Descent for random features </li>
</ul>
<b> Week 9: Neural Networks and Presentations </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 16: </b> Guest Lecture: M. Nica "Random Features and NTK </li>
  <li> <b> Presentation 1:</b> (M. Majid) Sum-of-Squares, UGC hardness, and Average case hardness 
</ul>
  <b> Week 10: Presentations </b>
<ul style="list-style-type:none;">
  <li> <b> Presentation 2: </b> (K. Ramsay) Benign overfitting in Linear regression</li>
  <li> <b> Lec 17</b> (joint with Probability seminar) Guest Lecture: M. Nica, Gradients for neural networks</li>
</ul>
<b> Week 11: Presentations</b>
<ul style="list-style-type:none;">
  <li> <b> Presentation 3:</b> (L. Zhang) Random matrix approach to neural nets</li>
  <li> <b> Presentation 4:</b> (A. Mouzakis) The Low Degree and Statistical Query frameworks </li></ul>
<b> Week 12: Wrap-up </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 18: </b>  So long, and thanks for all the fish </li> </ul>
    
  





  
	 	  

    <!--
<b>Evaluation:</b> <i>[Tentative]</i> The evaluation for this course will be based on problem sets,  a project, and participation.  The project can be either a literature review or original work and will have 3 components: (1) a brief proposal justifying your planned project, (2) a presentation followed by a Q+A with the instructor, (3) a final paper. The "participation" component will involve a joint effort to document the contents of the lecture.<br/><br/>

<b> Recommended reading:</b><br/><br/>
    <b> Textbooks: </b>
    <br/> Anderson,Guionnet, + Zeitouni "Random matrix theory"
      <br/> Ledoux "The concentration of measure phenomenon"
      <br/> Bucheron, Lugosi, + Massart, "Concentration Inequalities"
      <br/> M&eacute;zard + Montanari "Information, physics, and computation" 
      <br/> Versyhnin "High-dimensional probability with applications to data science " 
     <br/> Wainwright "High-dimensional statistics" 
<br/></br>

<b> Papers: </b><br/></br>
	-->	      


	
<!---
    
    <b>Note:</b> This course is officially listed as occuring in person. 
    This may change before the term begins or during the term depending on public health/university decisions and student input. In-class attendance is <b>not</b> required and the goal is to provide equitable access to course content (esp. the lecture) for those who do not come in person. The plan is to follow a hybrid model, where I live-stream lectures and provide some mechanism for real-time interaction from students not attending in person. If you are planning on taking this course reach out to me in advance to provide your input on the preferred mode of delivery. Auditing is not allowed this term. <br><br>
--->
<!--
<br>
<b> Course</b>: STAT 333 - Applied Probability (Section 002) <br><br>
<b> Section</b>: T-Th 11.30-12.50, First Class: Tues Jan 7<br><br>
<b> Room: </b> EIT 1015 <br><br>
<b> Office Hours: </b> T 4-5pm  [Prof Shen's and TA's office hours posted on Learn] <br><br>

See Learn for course information.
>



<!--
<br>
<b>Course</b>: MATH 275Z - Topics in Mean Field Spin Glasses <br><br>
<b>Section:</b> T-Th 10:00-11:30, First Class: Tues. Jan 23 <br><br>
<b>Room:</b> SC 221 <br><br>
<b>Office Hours:</b> T-Th 3.30-4.30pm <br><br> 
<b> Blurb: </b> 
<br>
<p> Historically, these models come from the study of statistical physics and have served as prototypical examples of complex energy landscapes. To tackle these questions statistical physicists developed a new class of tools, such as the cavity method and the replica symmetry breaking. Since their introduction, these methods have been applied to a wide variety of problems, from statistical physics, to combinatorics, to data science.

<p> In this course, we will study the mathematics literature surrounding these tools and their applications.  Some topics we’ll cover depending on time and interest will be: Guerra's RSB interpolation, characterization-by-invariance and Aizenmann—Sims—Starr schemes,  superconcentration, the complexity of random Morse functions, basic extreme value theory, exchangability, and universality. 
<br><br>



<b>Prerequisites:</b> I will assume a background in  measure theoretic probability or comparable knowledge of statistical physics<br><br>
 <br><br>

<a href="https://canvas.harvard.edu/courses/38676"> Course Website </a>

<br><br>
<b>Lectures:</b><br><br>
<b> Part I: The Simplest 1RSB system</b><br><br>
<b> Week 1</b> Introduction, Extreme Value Theory 101<br>
<b> Week 2</b> The Bolthausen&ndash;Sznitman invariance and  Poisson&ndash;Dirichlet processes <br><br>
<b> Part II: RSB interpolations </b> <br><br>
<b> Week 3</b> Comparison inequalities <br>
<b> Week 4</b> The RS bound (lec 1); Sketch of proof of Parisi formula <br>
<b> Week 5</b> Aizenman&ndash;Sims&ndash;Starr Scheme 1: Ruelle Probability Cascades and ROSt functionals<br>
<b> Week 6</b> Ghirlanda-Guerra Identities and their consequences <br> 
<b> Week 7</b> Panchenko's ultrametricity theorem and Proof of Parisi Formula  </br>
<b> Week 8</b> Spring Break </br><br>
<b> Part III: Consequences of the Parisi formula</b> <br><br>
<b> Week 9</b> Dynamic programming and Convexity of Parisi Functionals<br>
<b> Week 10</b> Variational methods I <br>
<b> Week 11</b> Variational methods II <br>
<br>
<b> Part IV:</b> Other perspectives<br><br>
<b>Week 11 (ct'd)</b> Dynamics  <br>
<b>Week 12</b> Dynamics  <br>
<b>Week 13</b> Disorder Chaos, Superconcentration, and  Multiple valleys; The replica derivation and recap   <br>
<b>Week 14</b> Guest lecture: S. Sen: Applications to graph optimization
<br><br>
<b>Recommended Reading: </b>
<ul>
<li> Ten Lectures on Disordered Media, E. Bolthausen and A.-S. Sznitman </li>
<li> Statistical Mechanics of Disordered Systems, A. Bovier </li>
<li> Information, Physics, and Computation, M. Mezard and A. Montanari </li>
<li> Spin Glass Theory and Beyond, M. Mezard, G. Parisi, and M. A. Virasoro </l>
<li> The Sherrington-Kirkpatrick Model, D. Panchenko</li>
<li> Mean Field Models for Spin Glasses Vol. 1-2, M. Talagrand</li>


</ul>
-->

</div>
</body>
</html>
