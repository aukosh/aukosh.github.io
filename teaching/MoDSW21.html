<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S79B41WR96"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S79B41WR96');
</script>

<meta  http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>
Aukosh Jagannath
</title>
<link rel="stylesheet" type="text/css" href="../jag.css" /> 
<!--
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
<?php
if( isset( $_COOKIE['user_is_admin12312341212'] ) ) {
    echo 'ga(\'set\', \'dimension1\', \'true\');';
}
?>
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
-->
<!--<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytic\
s.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

  </script>-->

</head>
<body>
<div class="container">

<div class="header">
<h1>
Aukosh Jagannath

</h1>
</div>
<div class = "menu">
<a href="./">home</a> &nbsp;
<a href="./research.html">research</a> &nbsp;
<a href="./people.html">people</a> &nbsp;
<a href="./teaching.html"><b>teaching</b></a> &nbsp;
<a href="./bio.html">bio</a> &nbsp;
<a href="./aukoshJagannathcv.pdf"> CV </a>

</div>





<br/>
<b> Course</b>: STAT 946 &mdash;  Mathematics of Data Science (Adv. Topics) <br/>
<b> Section</b>: Lec 001&mdash; Winter 2021<br/>
<!--<b> Location/Medium: </b> Live-stream <br/>
<b> Live-stream Times: </b> TTh 1-2.20pm EST (GMT-5)  <br/>
<b> Office Hours: </b> By appointment<br/>-->
<b> Course website: </b> See Learn.
<br/><br/>
 
    <b> Blurb: </b>
Data science is an exciting and multidisciplinary field which connects problems in a broad range of fields from statistics and probability, to computation and optimization, to harmonic analysis and dynamical systems. Given inference task, is it possible to compute your favorite estimator on a feasible timescale? Conversely, given a problem from machine learning, what are the fundamental statistical limits of your method? How do you begin to answer these questions when the dimension of the parameter space or the data set is extremely large and has complex (e.g., manifold, graph, group) structure? In these settings many of the classical tools from these fields do not apply and we have to revisit the mathematical foundations. <br/><br/>
  
<a href="notes/Stat946-MoDS.pdf"><b>Course Notes</b></a><br/><br/>
<!--
<b>Syllabus:</b> 

The goal of this course is to quickly orient you toward mathematical problems at the heart of data science. This course will focus on the statistical and computational limits of high-dimensional problems. The first third of the course will serve as an introduction to the basic tools of the course, namely high-dimensional probability and random matrix theory. We will then cover a broad range of applications. Depending on interest, applications may include:<br/>

<ul>
  <li>Dimension reduction</li>
  <li>Sparsity</li>
  <li>Low-rank models: the spiked matrix model for PCA and the BBP transition</li>
  <li> Community detection</li>
  <li> Mean-field methods: approximate message passing, belief propagation, et al </li>
  <li> Neural networks: mean-field limits and NTK for shallow networks, related kernel methods, approximation theory</li>
  <li>Stochastic approximation algorithms, Markov processes, and their mixing properties</li>
  <li>Random optimization problems</li>
  <li>Statistical phase transitions: Statistical—to—computational gaps</li>
</ul><br/>

  <b>Suggested Background:</b> This will be an advanced topics course and will be mathematically rigorous. We will focus on theoretical results regarding both statistical and computational limits. It is strongly recommended that the student have successfully completed at least one advanced course in probability, stochastic processes, or mathematical statistics, as well as courses in analysis and linear algebra. Please contact the instructor to confirm if your background is sufficient. <br/><br/>

<b> Lectures </b><br/><br/>
<b><u>Part I:</u> Fundamentals </b><br/><br/>
<b> Week 1: High-dimensional probability</b>
<ul style="list-style-type:none;">
  <li><b> Lec 1:</b> Surprises in high-dimensions,</li>
  <li><b> Lec 2:</b> Concentration of Gaussians</li>
</ul>
<b>Week 2: High-dimensional probability</b>
<ul style="list-style-type:none;">
  <li><b> Lec 3: </b> Concentration for (Gaussian) random matrices</li>
  <li><b> Lec 4: </b> Isoperimetric, Log-Sobolev, and Poincare inequalities </li>
</ul>
<b>Week 3: PCA and spiked matrix models </b>
<ul style="list-style-type:none;">
  <li><b> Lec 5: </b> Covariance estimation and spiked matrix models</li>
  <li><b> Lec 6: </b> Stieljes transforms and Wigner's theorem</li>
</ul>
<b> Week 4: Spiked matrix models </b>
<ul style="list-style-type:none;">
  <li><b> Lec 7: </b> The Marchenko&mdash;Pastur Law </li>
  <li><b> Lec 8: </b> The Baik-Ben Arous-Peche transition: a short proof 
</ul>

<b> Week 5: Gaussian processes and the M* bound</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 9: </b> Comparison Inequalities </li>
  <li><b> Lec 10: </b> Recovering a vector from a few random measurements </li>
</ul>
<b>Week 6: The escape theorem and exact recovery</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 10: </b> Recovering a vector from a few random measurements</li>
</ul><br/>

<b><u>Part II:</u> Selected recent results</b><br/><br/>
<b> Week 6: Mean-field methods I</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 11: </b>Approximate Message Passing in high-dimensional statistics</li>
</ul>
<b> Week 7: Mean-field methods II</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 12: </b> Approximate Message passing (ctd) </li>
  <li> <b> Lec 13:</b> Mean-field theories, MCMC, free energies </li>
</ul>
<b> Week 8: Neural Networks </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 14: </b> Neural Nets and Approximation theory </li>
  <li> <b> Lec 15:</b> Double Descent for random features </li>
</ul>
<b> Week 9: Neural Networks and Presentations </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 16: </b> Guest Lecture: M. Nica "Random Features and NTK </li>
  <li> <b> Presentation 1:</b> (M. Majid) Sum-of-Squares, UGC hardness, and Average case hardness 
</ul>
  <b> Week 10: Presentations </b>
<ul style="list-style-type:none;">
  <li> <b> Presentation 2: </b> (K. Ramsay) Benign overfitting in Linear regression</li>
  <li> <b> Lec 17</b> (joint with Probability seminar) Guest Lecture: M. Nica, Gradients for neural networks</li>
</ul>
<b> Week 11: Presentations</b>
<ul style="list-style-type:none;">
  <li> <b> Presentation 3:</b> (L. Zhang) Random matrix approach to neural nets</li>
  <li> <b> Presentation 4:</b> (A. Mouzakis) The Low Degree and Statistical Query frameworks </li></ul>
<b> Week 12: Wrap-up </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 18: </b>  So long, and thanks for all the fish </li> </ul>
    
-->


</div>
</body>
</html>
