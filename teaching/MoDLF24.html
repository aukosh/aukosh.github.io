<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S79B41WR96"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S79B41WR96');
</script>

<meta  http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>
Aukosh Jagannath
</title>
<link rel="stylesheet" type="text/css" href="../jag.css" /> 
<!--
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
<?php
if( isset( $_COOKIE['user_is_admin12312341212'] ) ) {
    echo 'ga(\'set\', \'dimension1\', \'true\');';
}
?>
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
-->
<!--<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytic\
s.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

  </script>-->

</head>
<body>
<div class="container">

<div class="header">
<h1>
Aukosh Jagannath

</h1>
</div>
<div class = "menu">
<a href="~/">home</a> &nbsp;
<a href="~/research.html">research</a> &nbsp;
<a href="~/people.html">people</a> &nbsp;
<a href="~/teaching.html"><b>teaching</b></a> &nbsp;
<a href="~/bio.html">bio</a> &nbsp;
<a href="~/aukoshJagannathcv.pdf"> CV </a>

</div>

<br>
<b> Course:</b> STAT 946 &mdash; Adv Topics: Mathematical Foundations of Deep Learning<br><br><br>

<b> Blurb:</b> 
  The goal of this course is to introduce and explore some of the recent theoretical advances that 
  aim to understand modern deep learning methods and training regimes. <br><br>

  Topics may include: Universal approximations, Uniform convergence, Benign overparamterization, functional limit theory/scaling limits,
  NTK, comparison to kernel methods, Training dynamics and related phenomenology (Implicit regularization, Training regimes, sample complexity, 
  Mean-Field/hydrodynamic limits, DMFT/Effective Dynamics), Loss landscapes and geometry, transformers, diffusion models.<br><br>

  There will be a heavy focus on the analysis of training dynamics (sample complexity and scaling limits).  
  The course will be interspersed with primers on important tools and techniques from probability theory such as 
  concentration of measure, random matrix theory, stochastic analysis, and related ideas inspired by statistical physics.<br><br>

  This will be a fast-paced, research-level, seminar-style course. We will be learning as a group. 
  Students will play an active role in the course: as the course progresses students will pick up and explore these 
  or other topics to catch us all up!
  <br><br>

The following is tentative and comments/suggested reading are welcome!<br><br>


<b><a href="~/notes/Stat946-MFDL.pdf">Course Notes</a></b> <br><br>

<b> Lectures</b><br> <br>
<b> Week 1:</b> Approximation theory<br>
  <ul style="list-style-type:none;">
    <li> <b> Lec 1:</b> Universal Approximation and Depth Separation </li>
    <li><b>Reading:</b> <a href=https://mjt.cs.illinois.edu/dlt/two.pdf>Telgarsky's Deep Learning Theory notes, Chap 1,4</a></li>
  </ul>
<b> Week 2:</b> Uniform convergence <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 2: </b> Generalization and Rademacher Complexity </li>
  <li> <b> Lec 3:</b> Vacuous bounds and the need for a new approach </li>
  <li><b>Reading:</b></li> 
  <ul>
    <li>(BMR) P. Bartlett, A Montanari, and A Rakhlin, <a href=https://www.cambridge.org/core/journals/acta-numerica/article/deep-learning-a-statistical-viewpoint/7BCB89D860CEDDD5726088FAD64F2A5A>
  Deep Learning: A statistical view point</a> Sec 2 </li>
   <!-- <li> M. Mohri, <a href=https://cs.nyu.edu/~mohri/mlbook/> Foundations of Machine Learning</a> Chap 3</li>-->
    <li> M. Wainwright, <a href="https://doi.org/10.1017/9781108627771">High-dimensional Statistics</a> Chap 4</li>
    <li> V. Nagarajan and J. Zico Kolter, <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf">Uniform convergence may be unable to explain generalization in deep learning</a>,
      NeurIPS 2019</li>
    <li> J. Negrea, G. K. Dziugaite, and D. Roy, <a href="http://proceedings.mlr.press/v119/negrea20a/negrea20a.pdf"> In Defense of Uniform Convergence</a> ICML 2020</li>
  <!--<li> S. Ben-David, S. Shalev-Schwarz "Understanding Machine Learning: From Theory to Algorithms" </li>-->
  </ul>
</ul>
<b> Week 3:</b> Implicit regularization and benign overparamterization<br>
<ul style="list-style-type:none;">
  <li> <b> Lec 4:</b> Implicit Regularization </li>
  <li> <b> Lec 5:</b> Interpolation does not imply poor generalzation </li>
  <li><b>Reading:</b></li>
  <ul>
    <li> Soudry-Hoffer-Nacson-Gunasekar-Srebro <a href="https://www.jmlr.org/papers/volume19/18-188/18-188.pdf">The Implicit Bias of Gradient Descent on Separable Data</a> JMLR 2018</li>
    <li> <a href=https://mjt.cs.illinois.edu/dlt/index.pdf>Telgarsky's Deep Learning Theory notes, Chap 10</a></li>
    <li> M. Belkin, A. Rakhlin, A.B. Tsybakov <a href="https://proceedings.mlr.press/v89/belkin19a.html">Does data interpolation contradict statistical optimality</a> AISTATS 2019</li>
  </ul>
</ul>
<b> Week 4:</b> RMT and double descent <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 6:</b> Random Matrix Theory: a primer</li>
  <li> <b> Lec 7:</b> High-dimensional ridgeless least squares</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>G. W. Anderson, A. Guionnet, and O. Zeitouni, <a href="http://www.wisdom.weizmann.ac.il/~zeitouni/cupbook.pdf"> An Introduction to Random Matrices</a></li>
    <li>T. Hastie, A. Montanari, S. Rosset, and RJ Tibshirani, <a href="https://arxiv.org/pdf/1903.08560">Surprises in High-dimensional Ridgeless least squares interpolation </a> Ann. Stats 2022</li>
  </ul>
</ul>
<b> Week 5:</b> NTK and Lazy training <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 8:</b> The Neural Tangent Kernel</li>
  <li> <b> Lec 9:</b> Lazy training</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>A. Jacot, F. Gabriel, and C. Hongler, <a href="https://papers.nips.cc/paper_files/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html"> Neural Tangent Kernel: Convergence and Generalization in Neural Networks</a> NeurIPS 2018</li>
    <li> BMR Sec 5 </li>
  </ul>
</ul>
<b> Week 6:</b> Infinite Width limits <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 10:</b> Guest Lecture</li>
  <li> <b> Lec 11:</b> Neural networks as interacting particle systems</li>

  <li><b>Reading:</b></li>
  <ul>
    <li>S. Mei, A. Montanari, P.-M. Nguyen,  <a href="https://web.stanford.edu/~montanar/RESEARCH/FILEPAP/mean_field.pdf"> A Mean Field View of the Landscape of Two-Layer Neural Networks </a> PNAS 2018</li>
  </ul>
</ul>
<b> Week 7: Reading Week</b> <br>
<b> Week 8:</b> Sample complexity and scaling limits <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 12:</b> Sample complexity and the information exponent</li>
  <li> <b> Lec 13:</b> Effective dynamics</li>
  <li><b>Reading:</b></li>
  <ul>
    <li> G. Ben Arous, R. Gheissari, A. Jagannath <a href="https://jmlr.org/papers/volume22/20-1288/20-1288.pdf">  Online stochastic gradient descent on non-convex losses from high-dimensional inference</a> JMLR 2021</li>
    <li> G. Ben Arous, R. Gheissari, A. Jagannath <a href = "https://onlinelibrary.wiley.com/doi/10.1002/cpa.22169">High-dimensional limit theorems for SGD: Effective dynamics and critical scaling</a> Comm Pure Appl Math 2024, NeurIPS 2022</li>
  </ul>
</ul>
<b> Week 9:</b> Spectral alignment and Transformers <br>
<ul style="list-style-type:none;">
  <li> <b> Lec 14:</b> Spectral alignment</li>
  <li> <b> Lec 15:</b> Transformers </li>
  <li><b>Reading:</b></li>
  <ul>
    <li> G. Ben Arous, R. Gheissari, J. Huang, A. Jagannath <a href="https://openreview.net/pdf?id=MHjigVnI04">High-dimensional SGD aligns with emerging outlier eigenspaces </a>  ICLR 2024 </li>
    <li> B. Geshkovski, C. Letrouit, Y. Polyanskiy, and P. Rigollet, <a href="https://arxiv.org/pdf/2312.10794"> A mathematical perspective on transformers</a></li>
  </ul>
</ul>
<b> Week 10:</b> CANCELLED <br>
<b> Week 11:</b> Diffusion Models + Student presentations<br>
<ul style="list-style-type:none;">
  <li> <b> Lec 16:</b> Diffusion models</li>
  <li> <b> Presentation 1:</b> Valentio, Learning GMMs using DDPM </li>
  <li><b>Reading:</b></li>
  <ul>
    <li> A. Montanari, <a href="https://arxiv.org/pdf/2305.10690"> Sampling, Diffusions, and Stochastic Localization</a></li>
    <li> K. Shah, S. Chen, A. Klivans, <a href="https://arxiv.org/pdf/2307.01178">Learning Mixtures of Gaussians Using the DDPM Objective </a> </li>
  </ul>
</ul>
<b> Week 12:</b> Student Presentations <br>
<ul style="list-style-type:none;">
  <li> <b> Presentation 2</b> Theo, Leap Complexity </li>
  <li> <b> Presentation 3</b> Varnan, Benefits of Reuse</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>TBA </li>
  </ul>
</ul>
<b> Week 13:</b> Student Presentations <br>
<ul style="list-style-type:none;">
  <li> <b> Presentation 4</b> Parsa, Scaling limits of GLMs</li>
  <li> <b> Presentation 5</b> Juju, Neural Covariance</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>TBA </li>
  </ul>
</ul>
<b> Week 14:</b> Student Presentations <br>
<ul style="list-style-type:none;">
  <li> <b> Presentation 6:</b> Sammy, Edge of Stability</li>
  <li><b>Reading:</b></li>
  <ul>
    <li>TBA</li>
  </ul>
</ul>



</div>
</body>
</html>
