<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S79B41WR96"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S79B41WR96');
</script>

<meta  http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>
Aukosh Jagannath
</title>
<link rel="stylesheet" type="text/css" href="../jag.css" /> 
<!--
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
<?php
if( isset( $_COOKIE['user_is_admin12312341212'] ) ) {
    echo 'ga(\'set\', \'dimension1\', \'true\');';
}
?>
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
-->
<!--<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31197629-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytic\
s.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

  </script>-->

</head>
<body>
<div class="container">

<div class="header">
<h1>
Aukosh Jagannath

</h1>
</div>
<div class = "menu">
<a href="./">home</a> &nbsp;
<a href="./research.html">research</a> &nbsp;
<a href="./teaching.html"><b>teaching</b></a> &nbsp;
<a href="./bio.html">bio</a> &nbsp;
<a href="./aukoshJagannathcv.pdf"> CV </a>

</div>





<br/>
<b> Course</b>: STAT 946 &mdash;  Mathematics of Data Science (Adv. Topics) <br/>
<b> Section</b>: Lec 001&mdash; Winter 2021<br/>
<!--<b> Location/Medium: </b> Live-stream <br/>
<b> Live-stream Times: </b> TTh 1-2.20pm EST (GMT-5)  <br/>
<b> Office Hours: </b> By appointment<br/>-->
<b> Course website: </b> See Learn.
<br/><br/>
 
    <b> Blurb: </b>
Data science is an exciting and multidisciplinary field which connects problems in a broad range of fields from statistics and probability, to computation and optimization, to harmonic analysis and dynamical systems. Given inference task, is it possible to compute your favorite estimator on a feasible timescale? Conversely, given a problem from machine learning, what are the fundamental statistical limits of your method? How do you begin to answer these questions when the dimension of the parameter space or the data set is extremely large and has complex (e.g., manifold, graph, group) structure? In these settings many of the classical tools from these fields do not apply and we have to revisit the mathematical foundations. <br/><br/>
  
<a href="notes/Stat946-MoDS.pdf"><b>Course Notes</b></a>
<b>Syllabus:</b> 

The goal of this course is to quickly orient you toward mathematical problems at the heart of data science. This course will focus on the statistical and computational limits of high-dimensional problems. The first third of the course will serve as an introduction to the basic tools of the course, namely high-dimensional probability and random matrix theory. We will then cover a broad range of applications. Depending on interest, applications may include:<br/>

<ul>
  <li>Dimension reduction</li>
  <li>Sparsity</li>
  <li>Low-rank models: the spiked matrix model for PCA and the BBP transition</li>
  <li> Community detection</li>
  <li> Mean-field methods: approximate message passing, belief propagation, et al </li>
  <li> Neural networks: mean-field limits and NTK for shallow networks, related kernel methods, approximation theory</li>
  <li>Stochastic approximation algorithms, Markov processes, and their mixing properties</li>
  <li>Random optimization problems</li>
  <li>Statistical phase transitions: Statistical—to—computational gaps</li>
</ul><br/>

  <b>Suggested Background:</b> This will be an advanced topics course and will be mathematically rigorous. We will focus on theoretical results regarding both statistical and computational limits. It is strongly recommended that the student have successfully completed at least one advanced course in probability, stochastic processes, or mathematical statistics, as well as courses in analysis and linear algebra. Please contact the instructor to confirm if your background is sufficient. <br/><br/>

<b> Lectures </b><br/><br/>
<b><u>Part I:</u> Fundamentals </b><br/><br/>
<b> Week 1: High-dimensional probability</b>
<ul style="list-style-type:none;">
  <li><b> Lec 1:</b> Surprises in high-dimensions,</li>
  <li><b> Lec 2:</b> Concentration of Gaussians</li>
</ul>
<b>Week 2: High-dimensional probability</b>
<ul style="list-style-type:none;">
  <li><b> Lec 3: </b> Concentration for (Gaussian) random matrices</li>
  <li><b> Lec 4: </b> Isoperimetric, Log-Sobolev, and Poincare inequalities </li>
</ul>
<b>Week 3: PCA and spiked matrix models </b>
<ul style="list-style-type:none;">
  <li><b> Lec 5: </b> Covariance estimation and spiked matrix models</li>
  <li><b> Lec 6: </b> Stieljes transforms and Wigner's theorem</li>
</ul>
<b> Week 4: Spiked matrix models </b>
<ul style="list-style-type:none;">
  <li><b> Lec 7: </b> The Marchenko&mdash;Pastur Law </li>
  <li><b> Lec 8: </b> The Baik-Ben Arous-Peche transition: a short proof 
</ul>

<b> Week 5: Gaussian processes and the M* bound</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 9: </b> Comparison Inequalities </li>
  <li><b> Lec 10: </b> Recovering a vector from a few random measurements </li>
</ul>
<b>Week 6: The escape theorem and exact recovery</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 10: </b> Recovering a vector from a few random measurements</li>
</ul><br/>

<b><u>Part II:</u> Selected recent results</b><br/><br/>
<b> Week 6: Mean-field methods I</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 11: </b>Approximate Message Passing in high-dimensional statistics</li>
</ul>
<b> Week 7: Mean-field methods II</b>
<ul style="list-style-type:none;">
  <li> <b> Lec 12: </b> Approximate Message passing (ctd) </li>
  <li> <b> Lec 13:</b> Mean-field theories, MCMC, free energies </li>
</ul>
<b> Week 8: Neural Networks </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 14: </b> Neural Nets and Approximation theory </li>
  <li> <b> Lec 15:</b> Double Descent for random features </li>
</ul>
<b> Week 9: Neural Networks and Presentations </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 16: </b> Guest Lecture: M. Nica "Random Features and NTK </li>
  <li> <b> Presentation 1:</b> (M. Majid) Sum-of-Squares, UGC hardness, and Average case hardness 
</ul>
  <b> Week 10: Presentations </b>
<ul style="list-style-type:none;">
  <li> <b> Presentation 2: </b> (K. Ramsay) Benign overfitting in Linear regression</li>
  <li> <b> Lec 17</b> (joint with Probability seminar) Guest Lecture: M. Nica, Gradients for neural networks</li>
</ul>
<b> Week 11: Presentations</b>
<ul style="list-style-type:none;">
  <li> <b> Presentation 3:</b> (L. Zhang) Random matrix approach to neural nets</li>
  <li> <b> Presentation 4:</b> (A. Mouzakis) The Low Degree and Statistical Query frameworks </li></ul>
<b> Week 12: Wrap-up </b>
<ul style="list-style-type:none;">
  <li> <b> Lec 18: </b>  So long, and thanks for all the fish </li> </ul>
    
  





  
	 	  

    <!--
<b>Evaluation:</b> <i>[Tentative]</i> The evaluation for this course will be based on problem sets,  a project, and participation.  The project can be either a literature review or original work and will have 3 components: (1) a brief proposal justifying your planned project, (2) a presentation followed by a Q+A with the instructor, (3) a final paper. The "participation" component will involve a joint effort to document the contents of the lecture.<br/><br/>

<b> Recommended reading:</b><br/><br/>
    <b> Textbooks: </b>
    <br/> Anderson,Guionnet, + Zeitouni "Random matrix theory"
      <br/> Ledoux "The concentration of measure phenomenon"
      <br/> Bucheron, Lugosi, + Massart, "Concentration Inequalities"
      <br/> M&eacute;zard + Montanari "Information, physics, and computation" 
      <br/> Versyhnin "High-dimensional probability with applications to data science " 
     <br/> Wainwright "High-dimensional statistics" 
<br/></br>

<b> Papers: </b><br/></br>
	-->	      


	
<!---
    
    <b>Note:</b> This course is officially listed as occuring in person. 
    This may change before the term begins or during the term depending on public health/university decisions and student input. In-class attendance is <b>not</b> required and the goal is to provide equitable access to course content (esp. the lecture) for those who do not come in person. The plan is to follow a hybrid model, where I live-stream lectures and provide some mechanism for real-time interaction from students not attending in person. If you are planning on taking this course reach out to me in advance to provide your input on the preferred mode of delivery. Auditing is not allowed this term. <br><br>
--->
<!--
<br>
<b> Course</b>: STAT 333 - Applied Probability (Section 002) <br><br>
<b> Section</b>: T-Th 11.30-12.50, First Class: Tues Jan 7<br><br>
<b> Room: </b> EIT 1015 <br><br>
<b> Office Hours: </b> T 4-5pm  [Prof Shen's and TA's office hours posted on Learn] <br><br>

See Learn for course information.
>



<!--
<br>
<b>Course</b>: MATH 275Z - Topics in Mean Field Spin Glasses <br><br>
<b>Section:</b> T-Th 10:00-11:30, First Class: Tues. Jan 23 <br><br>
<b>Room:</b> SC 221 <br><br>
<b>Office Hours:</b> T-Th 3.30-4.30pm <br><br> 
<b> Blurb: </b> 
<br>
<p> Historically, these models come from the study of statistical physics and have served as prototypical examples of complex energy landscapes. To tackle these questions statistical physicists developed a new class of tools, such as the cavity method and the replica symmetry breaking. Since their introduction, these methods have been applied to a wide variety of problems, from statistical physics, to combinatorics, to data science.

<p> In this course, we will study the mathematics literature surrounding these tools and their applications.  Some topics we’ll cover depending on time and interest will be: Guerra's RSB interpolation, characterization-by-invariance and Aizenmann—Sims—Starr schemes,  superconcentration, the complexity of random Morse functions, basic extreme value theory, exchangability, and universality. 
<br><br>



<b>Prerequisites:</b> I will assume a background in  measure theoretic probability or comparable knowledge of statistical physics<br><br>
 <br><br>

<a href="https://canvas.harvard.edu/courses/38676"> Course Website </a>

<br><br>
<b>Lectures:</b><br><br>
<b> Part I: The Simplest 1RSB system</b><br><br>
<b> Week 1</b> Introduction, Extreme Value Theory 101<br>
<b> Week 2</b> The Bolthausen&ndash;Sznitman invariance and  Poisson&ndash;Dirichlet processes <br><br>
<b> Part II: RSB interpolations </b> <br><br>
<b> Week 3</b> Comparison inequalities <br>
<b> Week 4</b> The RS bound (lec 1); Sketch of proof of Parisi formula <br>
<b> Week 5</b> Aizenman&ndash;Sims&ndash;Starr Scheme 1: Ruelle Probability Cascades and ROSt functionals<br>
<b> Week 6</b> Ghirlanda-Guerra Identities and their consequences <br> 
<b> Week 7</b> Panchenko's ultrametricity theorem and Proof of Parisi Formula  </br>
<b> Week 8</b> Spring Break </br><br>
<b> Part III: Consequences of the Parisi formula</b> <br><br>
<b> Week 9</b> Dynamic programming and Convexity of Parisi Functionals<br>
<b> Week 10</b> Variational methods I <br>
<b> Week 11</b> Variational methods II <br>
<br>
<b> Part IV:</b> Other perspectives<br><br>
<b>Week 11 (ct'd)</b> Dynamics  <br>
<b>Week 12</b> Dynamics  <br>
<b>Week 13</b> Disorder Chaos, Superconcentration, and  Multiple valleys; The replica derivation and recap   <br>
<b>Week 14</b> Guest lecture: S. Sen: Applications to graph optimization
<br><br>
<b>Recommended Reading: </b>
<ul>
<li> Ten Lectures on Disordered Media, E. Bolthausen and A.-S. Sznitman </li>
<li> Statistical Mechanics of Disordered Systems, A. Bovier </li>
<li> Information, Physics, and Computation, M. Mezard and A. Montanari </li>
<li> Spin Glass Theory and Beyond, M. Mezard, G. Parisi, and M. A. Virasoro </l>
<li> The Sherrington-Kirkpatrick Model, D. Panchenko</li>
<li> Mean Field Models for Spin Glasses Vol. 1-2, M. Talagrand</li>


</ul>
-->

</div>
</body>
</html>
